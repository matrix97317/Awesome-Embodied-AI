# Emodied-AI
This repository mainly organizes resources related to embodied intelligence, including data, models, hardware, and software infrastructure.

## Data
<details> <summary>3D Grounding</summary>

- SCENEVERSE: Scaling 3D Vision-Language Learning for Grounded Scene Understanding [[arxiv](https://arxiv.org/pdf/2401.09340.pdf)] [[github](https://scene-verse.github.io)]

</details>

<details> <summary>LLM-Robot</summary>

- CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks [[arxiv](https://arxiv.org/pdf/2112.03227.pdf)] [[github](https://github.com/mees/calvin/tree/main)]

</details>

## Model
<details> <summary>3D Grounding</summary>

- LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent [[arxiv](https://arxiv.org/pdf/2309.12311.pdf)] [[github](https://chat-with-nerf.github.io/)]

- Multi-View Transformer for 3D Visual Grounding [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Multi-View_Transformer_for_3D_Visual_Grounding_CVPR_2022_paper.pdf)] [[github](https://github.com/sega-hsj/MVT-3DVG)]

</details>
<details> <summary>LLM-Robot</summary>

- VISION-LANGUAGE FOUNDATION MODELS AS EFFEC-TIVE ROBOT IMITATORS [[arxiv](https://arxiv.org/abs/2311.01378.pdf)] [[github](roboflamingo.github.io)]

- MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World [[arxiv](https://arxiv.org/pdf/2401.08577.pdf)] [[website](https://vis-www.cs.umass.edu/multiply)]

</details>

<details> <summary>Task Planning</summary>

- Embodied Task Planning with Large Language Models [[arxiv](https://arxiv.org/pdf/2307.01848.pdf)] [[website](https://gary3410.github.io/TaPA/)]

</details>

## Hardware

## Software
